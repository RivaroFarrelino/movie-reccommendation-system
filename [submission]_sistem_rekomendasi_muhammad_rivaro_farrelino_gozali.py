# -*- coding: utf-8 -*-
"""[SUBMISSION]_Sistem Rekomendasi_Muhammad Rivaro Farrelino Gozali.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDZQ7a4TZiF7NywdppY6SLDgNpQPh4HV

Sumber Dataset: https://www.kaggle.com/code/rounakbanik/movie-recommender-systems#Content-Based-Recommender
"""

# datasets
! wget https://files.grouplens.org/datasets/movielens/ml-25m.zip

# unzip
! unzip ml-25m.zip -d /content/data/

"""## Import Library"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import datetime
import sys
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from sklearn.model_selection import train_test_split

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Load Data"""

# membaca dataset dan menyimpannya ke dalam variabel
movies = pd.read_csv('/content/data/ml-25m/movies.csv')         # dataset film
ratings = pd.read_csv('/content/data/ml-25m/ratings.csv')       # dataset rating film
tags = pd.read_csv('/content/data/ml-25m/tags.csv')             # dataset tag film
links = pd.read_csv('/content/data/ml-25m/links.csv')           # dataset link film
genome_tags = pd.read_csv('/content/data/ml-25m/genome-tags.csv')   # dataset tag genome film
genome_scores = pd.read_csv('/content/data/ml-25m/genome-scores.csv') # dataset skor relevansi tag

# menampilkan jumlah nilai unik dari masing-masing dataframe
print('Total film yang tersedia: ', len(movies.movieId.unique()))
print('Total pengguna yang memberikan rating: ', len(ratings.userId.unique()))
print('Total pengguna yang memberikan tag: ', len(tags.userId.unique()))
print('Total film yang memiliki link referensi: ', len(links.movieId.unique()))
print('Total tag unik pada genome tags: ', len(genome_tags.tagId.unique()))
print('Total film yang memiliki skor genome: ', len(genome_scores.movieId.unique()))

"""Membaca file CSV dataset ke DataFrame pandas untuk data film, rating, tag, link, genome tags dan scores. Dataset ini terdiri dari banyak file yang saling melengkapi untuk analisis rekomendasi.

## Data Inspection & Cleaning

Menampilkan info kolom, tipe data, dan nilai non-null untuk memastikan data lengkap dan mengetahui tipe data yang akan diproses.
"""

# informasi dataset movies
movies.info()

# informasi dataset ratings
ratings.info()

# informasi dataset tags
tags.info()

# informasi dataset links
links.info()

# informasi dataset genome_tags
genome_tags.info()

# informasi dataset genome_scores
genome_scores.info()

print('\nMissing values in movies dataset:')
print(movies.isnull().sum())

print('\nMissing values in ratings dataset:')
print(ratings.isnull().sum())

print('\nMissing values in tags dataset:')
print(tags.isnull().sum())

print('\nMissing values in links dataset:')
print(links.isnull().sum())

print('\nMissing values in genome_tags dataset:')
print(genome_tags.isnull().sum())

print('\nMissing values in genome_scores dataset:')
print(genome_scores.isnull().sum())

"""Menggabungkan dan menghapus duplikasi tagId, movieId, dan userId dari dataset yang berbeda untuk mendapatkan total keseluruhan unik. Memastikan semua data konsisten dan lengkap."""

# Mengambil semua tagId unik dari kedua dataset genome_tags dan genome_scores, lalu digabungkan
genome_tags_all = np.concatenate((genome_tags.tagId.unique(), genome_scores.tagId.unique()))

# Menghapus duplikasi dan mengurutkan tagId hasil gabungan
genome_tags_all = np.sort(np.unique(genome_tags_all))

# Menampilkan jumlah total tagId unik setelah digabung dan disortir
print('Jumlah seluruh data genome_tags: ', len(genome_tags_all))

# Mengambil semua movieId unik dari keempat dataset: movies, tags, ratings, dan links, lalu digabungkan
movies_all = np.concatenate((
    movies.movieId.unique(),
    tags.movieId.unique(),
    ratings.movieId.unique(),
    links.movieId.unique()
))

# Menghapus duplikasi movieId dan mengurutkan hasilnya
movies_all = np.sort(np.unique(movies_all))

# Menampilkan jumlah total movieId unik setelah digabung dan disortir
print('Jumlah seluruh data movies berdasarkan movieId: ', len(movies_all))

# Mengambil semua userId unik dari dataset ratings dan tags, lalu digabungkan
user_all = np.concatenate((
    ratings.userId.unique(),
    tags.userId.unique()
))

# Menghapus duplikasi userId dan mengurutkan hasil gabungan
user_all = np.sort(np.unique(user_all))

# Menampilkan jumlah total userId unik setelah digabung dan disortir
print('Jumlah seluruh data user berdasarkan userId: ', len(user_all))

# dataset movies
movies

"""* Mengambil tahun rilis film dari judul (biasanya di akhir dalam tanda kurung).

- Menghapus tahun dari judul agar fokus pada nama film saja saat analisis teks/genre.

- Membersihkan data untuk fitur yang lebih baik.
"""

# Mengekstrak tahun rilis dari kolom title dan menyimpannya ke kolom baru 'year_of_release'
movies['year_of_release'] = movies.title.str.extract('([0-9]{4})')
movies.head()

# Mengubah tipe data kolom 'title' menjadi string
movies['title'] = movies['title'].astype(str)

# Menghapus informasi tahun (atau teks setelah tanda kurung buka) dari judul film
movies['title'] = movies['title'].str.split('(', n=1).str[0].str.strip()

movies.head()

ratings

ratings.rating.unique()

"""Membulatkan nilai rating ke atas agar rating menjadi integer (1-5). Kemudian mengonversi kolom timestamp ke format datetime agar lebih mudah dianalisis."""

# Membulatkan nilai pada kolom 'rating' ke atas (ceil)
ratings['rating'] = ratings['rating'].apply(np.ceil)

# Memeriksa kembali nilai-nilai unik yang ada di kolom 'rating'
ratings.rating.unique()

ratings.timestamp = pd.to_datetime(ratings['timestamp'], unit='s')
ratings.head()

"""Menggabungkan data film dengan rating berdasarkan movieId. Menghapus nilai missing agar data bersih untuk pemodelan."""

# Menggabungkan dataframe movies dan ratings berdasarkan kolom 'movieId' dengan metode left join
films = pd.merge(movies, ratings, on='movieId', how='left')

# Menampilkan dataframe hasil penggabungan
films

# check missing values
(films.isnull() | films.empty | films.isna()).sum()

# drop missing value
films = films.dropna()
films

# check ulang missing values
(films.isnull() | films.empty | films.isna()).sum()

"""## Exploratory Data Analysis (EDA)

Melihat berapa genre unik dan jenis-jenis genre film yang tersedia. Menghapus film yang tidak memiliki genre.
"""

# Set opsi cetak NumPy agar seluruh elemen array ditampilkan tanpa dipersingkat
np.set_printoptions(threshold=sys.maxsize)

# Cetak jumlah genre unik yang ada di kolom 'genres' pada dataframe films
print('Banyak genre films: ', len(films['genres'].unique()))

# Cetak daftar lengkap genre unik pada kolom 'genres'
print('Genre films: ', films['genres'].unique())

# Kembalikan opsi cetak NumPy ke pengaturan default yang mempersingkat output panjang
np.set_printoptions(threshold=None)

# show non-genre
films[films['genres']=='(no genres listed)']

films = films.loc[films.genres != '(no genres listed)']
films.head()

films.groupby('movieId').sum(numeric_only=True)

"""Memilih film yang memiliki setidaknya 50 rating agar model fokus pada data yang cukup banyak. Menghapus duplikasi berdasarkan filmId dan judul. Membersihkan genre "Sci-Fi" agar konsisten penulisannya.


"""

# Menghitung frekuensi kemunculan setiap movieId di dataframe films
get_values = films['movieId'].value_counts()

# Memilih movieId yang muncul setidaknya 50 kali
temp = get_values[get_values >= 50].index

# Memfilter dataframe films agar hanya menyimpan baris dengan movieId yang frekuensinya >= 50
films = films[films['movieId'].isin(temp)]

# Menampilkan dataframe hasil filter
films

# duplicated by movieId
films.duplicated('movieId').sum()

# duplicated by title
films.duplicated('title').sum()

# drop duplicated data by movieId & title
films = films.drop_duplicates('movieId')
films = films.drop_duplicates('title')

# Ganti string yang cocok dengan pola regex '[nS]ci-Fi' menjadi 'Scifi' di kolom 'genres'
films['genres'] = films['genres'].str.replace('[nS]ci-Fi', 'Scifi', regex=True)

films.head()

"""## Feature Engineering (TF-IDF Vectorizer & Cosine Similarity)"""

# Membuat salinan dataframe films ke variabel preparation
preparation = films
preparation.sort_values(by='movieId').head()

# Mengubah kolom 'movieId' pada dataframe preparation menjadi list
film_id = preparation['movieId'].to_list()

# Mengubah kolom 'title' pada dataframe preparation menjadi list
film_name = preparation['title'].to_list()

# Mengubah kolom 'genres' pada dataframe preparation menjadi list
film_genre = preparation['genres'].to_list()

# Menampilkan panjang masing-masing list
print(len(film_id))
print(len(film_name))
print(len(film_genre))

# Membuat dataframe dari dictionary yang berisi list film_id, film_name, dan film_genre
df_film = pd.DataFrame(data={
    'film_id': film_id,
    'film_name': film_name,
    'genre': film_genre
})
df_film

# data sample
data = df_film
data.head()

"""## Content-based Filtering

Menggunakan TF-IDF untuk mengubah kolom genre menjadi representasi numerik (vektor) yang menunjukkan bobot penting genre untuk setiap film. Digunakan untuk mengukur kesamaan antar film berdasarkan genre.
"""

# Membuat objek TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Melatih model TF-IDF pada kolom 'genre' dari dataframe data
tfidf_vectorizer.fit(data['genre'])

# Mendapatkan daftar fitur (kata) yang dihasilkan oleh TfidfVectorizer
tfidf_vectorizer.get_feature_names_out()

# fit & transform to matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(data['genre'])

# show matrix dimension
tfidf_matrix.shape

# Mengubah vektor tf-idf menjadi bentuk matriks padat (dense matrix)
tfidf_matrix.todense()

# Membuat dataframe dari matriks tf-idf dengan baris sebagai film_name dan kolom sebagai genre
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf_vectorizer.get_feature_names_out(),
    index=data.film_name
).sample(n=20, axis=1).sample(n=10, axis=0)

print(tfidf_matrix.shape)
print(type(tfidf_matrix))

"""*Dislaimer: dikarenakan keterbatasan RAM Google Collab yang saya miliki, maka dari itu untuk pengambilan subset data yang dihitung akan dibatasi*"""

# total data
n = tfidf_matrix.shape[0]

# ambil 10000 indeks acak tanpa pengulangan
random_indices = np.random.choice(n, size=10000, replace=False)

# ambil subset sparse matrix berdasarkan indeks acak
subset_matrix = tfidf_matrix[random_indices]

# hitung cosine similarity pada subset acak ini
cosine_sim_subset = cosine_similarity(subset_matrix)

# contoh: film nama yang diambil sesuai index acak
subset_film_names = data.film_name.iloc[random_indices].tolist()

cosine_sim_df = pd.DataFrame(
    cosine_sim_subset,
    index=subset_film_names,
    columns=subset_film_names
)
print('Shape:', cosine_sim_df.shape)
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Fungsi yang mengambil nama film dan memberikan rekomendasi film serupa berdasarkan similarity matrix. Mengambil film dengan skor kemiripan tertinggi."""

# function recommendations
def film_recommendations(film_name, similarity_data=cosine_sim_df, items=data[['film_name', 'genre']], k=5):
    # urutkan skor kemiripan dari tinggi ke rendah (tanpa film itu sendiri)
    similarity_scores = similarity_data[film_name].sort_values(ascending=False)

    # ambil top-k+1 teratas karena termasuk film itu sendiri
    closest = similarity_scores.iloc[1:k+1].index

    # gabungkan dengan data film untuk mendapatkan genre
    return pd.DataFrame(closest, columns=['film_name']).merge(items).head(k)

# sample data
data.sample(3)

data[data.film_name.eq('The Raid 2: Berandal')]

film_recommendations('The Raid 2: Berandal')

df = preparation
df

"""## Collaborative Filtering

Mengubah ID asli user dan film menjadi angka berurutan untuk diproses dalam embedding model. Ini penting untuk input layer embedding pada neural network.
"""

# ubah nilai unik dari 'userId' menjadi list
user_ids = list(set(df['userId']))
print('list userID: ', user_ids)

# encode 'userId'
user_to_user_encoded = dict(zip(user_ids, range(len(user_ids))))
print('encoded userID : ', user_to_user_encoded)

# encoding index ke 'userId'
user_encoded_to_user = dict(zip(range(len(user_ids)), user_ids))
print('encoded angka ke userID: ', user_encoded_to_user)

# ubah nilai unik dari 'movieId' menjadi list
films_ids = list(set(df['movieId']))

# encode 'movieId'
films_to_films_encoded = dict(zip(films_ids, range(len(films_ids))))

# encoding index ke 'movieId'
films_encoded_to_films = dict(zip(range(len(films_ids)), films_ids))

# Menambahkan kolom 'user' berdasarkan hasil encoding 'userId'
df.loc[:, 'user'] = df['userId'].apply(lambda x: user_to_user_encoded[x])

# Menambahkan kolom 'films' berdasarkan hasil encoding 'movieId'
df.loc[:, 'films'] = df['movieId'].apply(lambda x: films_to_films_encoded[x])

# Menghitung jumlah user unik
num_users = df['user'].nunique()
print(num_users)

# Menghitung jumlah film unik
num_films = df['films'].nunique()
print(num_films)

# Mengubah tipe data rating menjadi float32
df['rating'] = df['rating'].astype('float32')

# Mengambil nilai rating minimum dan maksimum
min_rating = df['rating'].min()
max_rating = df['rating'].max()

# Menampilkan informasi jumlah user, film, dan nilai rating
print(f'Number of User: {num_users}, Number of Films: {num_films}, Min Rating: {min_rating}, Max Rating: {max_rating}')

"""**Split Data for Training and Validation**

melakukan sampling dan pembagian data menjadi data training dan validasi dgn proporsi 80/20
"""

# sampling
df = df.sample(frac=1, random_state=42)
df

# Mengambil fitur user dan films
x = df[['user', 'films']].values

# Normalisasi rating ke rentang 0-1
y = df['rating'].apply(lambda r: (r - min_rating) / (max_rating - min_rating)).values

# Membagi data menjadi training dan validation dengan proporsi 80:20 secara acak
x_train, x_val, y_train, y_val = train_test_split(
    x, y, test_size=0.2, random_state=42, shuffle=True
)

print(x_train.shape, y_train.shape)
print(x_val.shape, y_val.shape)

"""### Proses Training Model"""

class RecommenderNet(tf.keras.Model):

    def __init__(self, num_users, num_films, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users
        self.num_films = num_films
        self.embedding_size = embedding_size

        # User embedding dan bias
        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        # Film embedding dan bias
        self.films_embedding = layers.Embedding(
            input_dim=num_films,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.films_bias = layers.Embedding(num_films, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        films_vector = self.films_embedding(inputs[:, 1])
        films_bias = self.films_bias(inputs[:, 1])

        # Menghitung hasil dot product antara user dan film embedding
        dot_product = tf.reduce_sum(user_vector * films_vector, axis=1, keepdims=True)

        # Menjumlahkan dot product dengan bias user dan film
        x = dot_product + user_bias + films_bias

        return tf.nn.sigmoid(x)

"""Model di compile dengan menggunakan BinaryCrossentropy sebagai fungsi loss, Adam (Adaptive Moment Estimation) sebagai optimizer, serta root mean squared error (RMSE) sebagai metrik evaluasi."""

model = RecommenderNet(num_users, num_films, 50) # model initialization

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""Memvisualisasikan perkembangan error pada data training dan validasi. Membantu melihat apakah model overfitting atau underfitting."""

# plot metrics evaluations
plt.plot(history.history['root_mean_squared_error'], color='blue')
plt.plot(history.history['val_root_mean_squared_error'], color='red')
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""dari visualisasi ini, didapatkan hasil error akhir sebesar 0.03 dan error pada data validasi sebesar 0.25"""

# films
films_df = df_film
films_df.head()

# data ratings
df = films
df.head()

"""- Memilih pengguna acak.
- Mengambil film yang belum pernah ditonton user tersebut.
- Melakukan prediksi rating terhadap film tersebut.
- Mengambil 10 film dengan prediksi rating tertinggi sebagai rekomendasi.
"""

# Mengambil satu sampel user secara acak
user_id = df['userId'].sample(n=1).iloc[0]

# Film yang sudah ditonton oleh user tersebut
films_visited_by_user = df[df['userId'] == user_id]

# Film yang belum ditonton user (menggunakan operator ~ untuk negasi)
films_not_visited = films_df.loc[
    ~films_df['film_id'].isin(films_visited_by_user['movieId'].values), 'film_id'
]

# Filter film yang ada di encoding film
films_not_visited = list(set(films_not_visited).intersection(set(films_to_films_encoded.keys())))

# Membuat list encoded film yang belum ditonton
films_not_visited = [[films_to_films_encoded[x]] for x in films_not_visited]

# Mengambil encoded user
user_encoder = user_to_user_encoded[user_id]

# Membuat array kombinasi user dengan semua film yang belum ditonton
user_films_array = np.hstack((
    np.full((len(films_not_visited), 1), user_encoder),
    films_not_visited
))

# Prediksi rating film yang belum ditonton user
ratings = model.predict(user_films_array).flatten()

# Ambil indeks 10 rating tertinggi secara terbalik (descending)
top_ratings_indices = np.argsort(ratings)[-10:][::-1]

# Mapping indeks ke film_id asli
recommended_films_ids = [films_encoded_to_films[films_not_visited[i][0]] for i in top_ratings_indices]

print(f"Showing recommendations for users: {user_id}")
print("===" * 9)
print("Films with high ratings from user")
print("----" * 8)

# Ambil 5 film dengan rating tertinggi yang sudah ditonton user
top_films_user = films_visited_by_user.sort_values(by='rating', ascending=False).head(5)['movieId'].values

# Tampilkan nama dan genre film yang sudah ditonton
for _, row in films_df[films_df['film_id'].isin(top_films_user)].iterrows():
    print(f"{row.film_name} : {row.genre}")

print("----" * 8)
print("Top 10 films recommendation")
print("----" * 8)

# Tampilkan rekomendasi film berdasarkan prediksi model
for _, row in films_df[films_df['film_id'].isin(recommended_films_ids)].iterrows():
    print(f"{row.film_name} : {row.genre}")